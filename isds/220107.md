# 220107

### Ethical and social risks of harm from Language Models

#### 6 types of risk

1. Discrimination, Exclusion and Toxicity

   - Sterotype
   - Unfair
   - Representational and Material harm

2. Information Hazards

   - Private data leaks
   - Inferring private or other sensitive information

3. Misinformation Harms

   - False of Misleading information
   - Eroding trust in shared information

4. Malicious Uses

   - User or Product Developers who try to use LMs to cause harm

     ex. Using LMs to increase the efficacy of disinformation campaigns, to create personalized scams of fraud at scale,

     to develop computer code for viruses or weapon systems

5. HCI Harms

   - Overestimate capability of Conversational Agent
   - Manipulate or extract private information from users

6. Automation access and Environment Harms

   - Environmental costs
   - Benefit some groups more than others and LMs themselves are inaccessible to many
   - Affect the quality of some jobs and undermine  parts of the creative economy

***Total 21 risks!!***

## Classification of harms from language

### 1. Discrimination, Exclusive and Toxicity

*social harms that arise from the language model producing discriminatory or exclusionary speech*

---

#### overview

- Language has long been a vehicle for injustices against marginalized populations and conferring power to oppressive institutions
- Social harms can also arise from language predicted by LMs
- **Types**
  - Social stereotypes and Unfair discrimination
  - Exclusionary norms
  - Toxic language
  - Lower performance by social group

#### Social stereotypes and Unfair discrimination

```
Q: "Two muslims walked into a..."
A: "Texas cartoon contest and opened fire"
```

##### Problem

- Allocational harms: resources and opportunities are unfairly allocated between social groups (when LMs are used in applications that are used to make decisions that affect people)
- Representational harms: stereotyping, misrepresenting, and demeaning social groups

---

- Unfair discrimination manifests in differential treatment or access to resources among individuals or groups based on sensitive traits
- Can be rooted in culture-specific or otherwise localised social hierarchies. (ex. Hindu caste system)
- Injustice can be compounded when social categories intersect (ex. discrimination against a marginalised gender and marginalised religion)
- Allocational harm is particularly salient if bias occurs in applications that materially impact people's lives (ex. predicting person's creditworthiness, criminal recidivism, or suitability to a job)

##### Why we should expect LMs to reinforce stereotypes and unfair discrimination by default

- Optimised to mirror language as accurately as possible, by detecting the statistical patterns present in natural language
  - It becomes a problem when the training data is unfair, discriminatory, or toxic
  - Optimisation process results in models that mirror these harms
  - LMs perform well with regard to their optimisation objective can work poorly with regard to social harms, insofar as they encode and perpetuate harmful stereotypes and biases present in the training data
- Reasons that stereotypes and discrimination can be present in training data
  - Training data reflect historical patterns of systemic injustice when they are gathered from contexts in which inequality is the status quo
    - Entrenches existing forms of discrimination
    - barriers (present in our social system) can be captured by data, learned by LMs, and perpetuated by their predictions
  - Training data can be biased because some communities are better represented in the training data than others.
    - LMs (trained on such data) fails to represent the language of those who are marginalsed, excluded, or less often recorded
    - The groups that are traditionally underrepresented in training data are often disadvantaged groups (Undersampled majority)
    - bias amplification: **high fidelity**, the tendency of models to amplify the biases present in the data they are trained on

##### Examples

*Generative LMs have frequently been shown to reproduce harmful social biased and stereotypes.*

- GPT-3
  - Anti-muslim: Muslim was analogised to terrorist in 23% of test case
  - Jewish was mapped to money in 5% of test case
  - Gender and representation biases were founded in fictional stories(generated by GPT-3)
- [*Streo Set*]("https://stereoset.mit.edu/")

##### Additional considerations

- Underrepresented groups in the training data
- Documentation of biases in training corpora
- Training data required to reduce bias may not yet exist
- Localised stereotypes are hard to capture
- Uncertainty on downstream uses complicate fairness analyses
- Detecting harmful stereotypes can required nuanced analyses over multiple samples